{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pretraining_a_RoBERT_Model_from_Scratch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtCQAk5ps461"
      },
      "source": [
        "# Pretraining a RoBERTa Model from Scratch\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxop6UrAvVrl"
      },
      "source": [
        "## Training a tokenizer and pretraining a transformer\n",
        "\n",
        "Dans ce guide, nous allons entra√Æner un mod√®le de transformateur nomm√© KantaiBERT en utilisant les blocs de construction fournis par Hugging Face pour les mod√®les de type BERT. Nous avons couvert la th√©orie des √©l√©ments constitutifs du mod√®le que nous avons utilis√© pr√©c√©dement.\n",
        "\n",
        "Nous d√©crirons KantaiBERT en nous appuyant sur les connaissances acquises dans les guides pr√©c√©dents\n",
        "\n",
        "KantaiBERT est un mod√®le de type Robustly Optimized BERT Pretraining Approach (RoBERTa) bas√© sur l'architecture de BERT. \n",
        "\n",
        "\n",
        "Les mod√®les BERT initiaux √©taient sous-form√©s. RoBERTa augmente les performances des transformateurs de pr√©-apprentissage pour les t√¢ches en aval. RoBERTa a am√©lior√© la m√©canique du processus de pr√©-formation. Par exemple, il n'utilise pas la tokenisation WordPiece mais descend √† l'encodageByte Pair Encoding (BPE)\n",
        "\n",
        "Dans ce guide, KantaiBERT, comme BERT, sera form√© √† l'aide de la mod√©lisation du langage masqu√©. KantaiBERT sera form√© comme un petit mod√®le avec 6 couches, 12 t√™tes et 84 095 008 param√®tres. Il peut sembler que 84 millions de param√®tres repr√©sentent un grand nombre de param√®tres. \n",
        "\n",
        "Cependant, les param√®tres sont r√©partis sur 6 couches et 12 t√™tes, ce qui le rend relativement petit. Un petit mod√®le rendra l'exp√©rience de pr√©-entra√Ænement fluide afin que chaque √©tape puisse √™tre visualis√©e en temps r√©el sans attendre des heures pour voir un r√©sultat\n",
        "\n",
        "\n",
        "KantaiBERT est un mod√®le de type DistilBERT car il a la m√™me architecture de 6 couches et 12 t√™tes. DistilBERT est une version distill√©e de BERT. Nous savons que les grands mod√®les offrent d'excellentes performances. Mais que faire si vous souhaitez ex√©cuter un mod√®le sur un smartphone ? La miniaturisation a √©t√© la cl√© de l'√©volution technologique. \n",
        "\n",
        "Les transformateurs devront suivre le m√™me chemin lors de la mise en ≈ìuvre. L'approche Hugging Face utilisant une version distill√©e de BERT est donc un bon pas en avant. La distillation, ou d'autres m√©thodes de ce type √† l'avenir, est un moyen intelligent de tirer le meilleur parti de la pr√©formation et de la rendre efcace pour les besoins de nombreuses t√¢ches en aval.\n",
        "\n",
        "KantaiBERT impl√©mentera un tokenizer byte-level byte-pair encoding comme celui utilis√© par GPT-2. Les jetons sp√©ciaux seront ceux utilis√©s par RoBERTa.\n",
        "\n",
        "\n",
        "Il n'y a pas d'ID de type de jeton pour indiquer √† quelle partie d'un segment un jeton fait partie. Les segments seront s√©par√©s avec le jeton de s√©paration \\</s>.\n",
        "\n",
        "KantaiBERT utilisera un ensemble de donn√©es personnalis√©, formera un tokenizer, formera le mod√®le de transformateur, l'enregistrera et l'ex√©cutera avec un exemple de mod√©lisation de langage masqu√©.\n",
        "\n",
        "Commen√ßons √† construire un transformateur de z√©ro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzuVQsbsyqxP"
      },
      "source": [
        "## √âtape 1 : Chargement de l'ensemble de donn√©es \n",
        "\n",
        "Les ensembles de donn√©es pr√™ts √† l'emploi offrent un moyen objectif d'entra√Æner et de comparer les transformateurs. \n",
        " \n",
        " Le but de ce guide est de comprendre le processus d'apprentissage d'un transformateur avec des cellules de bloc-notes qui pourrait √™tre ex√©cut√© en temps r√©el sans avoir √† attendre des heures pour obtenir un r√©sultat.\n",
        "\n",
        " \n",
        "J'ai choisi d'utiliser les ≈ìuvres d'Emmanuel Kant (1724-1804), le philosophe allemand, qui fut l'incarnation du si√®cle des Lumi√®res. L'id√©e est d'introduire une logique humaine et un raisonnement pr√©-entra√Æn√© pour les t√¢ches de raisonnement en aval.\n",
        "\n",
        "\n",
        "Project Gutenberg, https://www.gutenberg.org, propose une large gamme de livres √©lectroniques gratuits qui peuvent √™tre t√©l√©charg√©s au format texte. \n",
        "\n",
        "Vous pouvez utiliser d'autres livres si vous souhaitez cr√©er vos propres ensembles de donn√©es personnalis√©s bas√©s sur des livres. J'ai compil√© les trois livres suivants d'Immanuel Kant dans un fichier texte nomm√© kant.txt :\n",
        "\n",
        "\n",
        "* he Critique of Pure Reason\n",
        "* The Critique of Practical Reason\n",
        "* Fundamental Principles of the Metaphysic of Morals\n",
        "\n",
        "kant.txt fournit un petit ensemble de donn√©es d'entra√Ænement pour entra√Æner le mod√®le de transformateur de ce guide. \n",
        "\n",
        "Le r√©sultat obtenu reste exp√©rimental. Pour un projet r√©el.\n",
        "\n",
        "L'ensemble de donn√©es est t√©l√©charg√© automatiquement depuis GitHub :\n",
        "\n",
        "vous pouvez utiliser curl pour le r√©cup√©rer depuis GitHub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDddOVc00CBU",
        "outputId": "5362acbd-7161-4737-e4c7-c12ed58aa6b8"
      },
      "source": [
        "!curl -L https://raw.githubusercontent.com/PacktPublishing/Transformers-for-Natural-Language-Processing/master/Chapter03/kant.txt --output \"kant.txt\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 10.7M  100 10.7M    0     0  20.5M      0 --:--:-- --:--:-- --:--:-- 20.4M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJdyXdIW0Qrc"
      },
      "source": [
        "## Step 2: Installation Hugging Face transformersWe \n",
        "\n",
        "Nous devrons installer des transformateurs et des tokenizers Hugging Face, mais nous n'aurons pas besoin de TensorFlow dans cette instance de la VM Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzyKfqoK0pkf",
        "outputId": "feaf9c32-5f76-4bee-e0ea-b40343ae1bf9"
      },
      "source": [
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip list | grep -E 'transformers|tokenizers'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-wnwaxpi1\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-wnwaxpi1\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 895 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (1.19.5)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.3 MB 31.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (3.3.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 596 kB 44.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (4.8.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (21.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (2019.12.20)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.1.1-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 59 kB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.13.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.13.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.13.0.dev0) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13.0.dev0) (7.1.2)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.13.0.dev0-py3-none-any.whl size=3101540 sha256=c578e02210150d279d091b52e3928a5730f8ede84d27c5bae83a1d270793cf7d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-snvj8l71/wheels/35/2e/a7/d819e3310040329f0f47e57c9e3e7a7338aa5e74c49acfe522\n",
            "Successfully built transformers\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.1.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.13.0.dev0\n",
            "tokenizers                    0.10.3\n",
            "transformers                  4.13.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjFGMp2T02_x"
      },
      "source": [
        "## √âtape 3 : Formation d'un tokenizer\n",
        "\n",
        "Dans cette section, le programme n'utilise pas de tokenizer pr√©-entra√Æn√©. Par exemple, un tokenizer GPT-2 pr√©-entra√Æn√© pourrait √™tre utilis√©. Cependant, le processus de formation de ce chapitre comprend la formation d'un tokenizer √† partir de z√©ro. ByteLevelBPETokenizer() de Hugging Face sera form√© √† l'aide de kant.txt. \n",
        "\n",
        "Un tokenizer au niveau de l'octet divisera une cha√Æne ou un mot en une sous-cha√Æne ou un sous-mot. Il y a deux avantages principaux parmi tant d'autres\n",
        "\n",
        "* Le tokenizer peut diviser les mots en composants minimaux. Ensuite, il fusionnera ces petits composants en d'autres statistiquement int√©ressants. Par exemple, \"smaller\" et smallest\" peuvent devenir \"small\", \"er\" et \"est\".\n",
        "\n",
        "Le tokenizer peut aller plus loin, et on pourrait obtenir ¬´ sm ¬ª et ¬´ all ¬ª, par exemple. Dans tous les cas, les mots sont d√©compos√©s en jetons de sous-mots et en unit√©s plus petites de parties de sous-mots telles que \"sm\" et \"all\" au lieu de simplement \"small\".\n",
        "\n",
        "* Les morceaux de cha√Ænes class√©s comme un jeton inconnu, utilisant l'encodage de niveau WorkPiece, dispara√Ætront pratiquement.\n",
        "\n",
        "\n",
        "Dans ce mod√®le, nous allons entra√Æner le tokenizer avec les param√®tres suivants :\n",
        "\n",
        "* files=paths est le chemin d'acc√®s √† l'ensemble de donn√©es.\n",
        "* vocab_size=52_000 est la taille de la longueur du mod√®le de notre tokenizer‚Ä¢\n",
        "* min_fr√©quence=2 est le seuil de fr√©quence minimum.\n",
        "* special_tokens=[] est une liste de jetons sp√©ciaux\n",
        "\n",
        "\n",
        "Dans ce cas, la liste des jetons sp√©ciaux est :\n",
        "* \\<s> : un jeton de d√©but\n",
        "* \\<pad> : un jeton de remplissage\n",
        "* \\</s> : un jeton de fin\n",
        "* \\<unk> : un jeton inconnu\n",
        "* \\<mask> : le jeton de masque pour la mod√©lisation du langage\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9u4scPP3-q3"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "paths = [str(x) for x in Path(\".\").glob(\"**/*.txt\")]\n",
        "\n",
        "# Initialize a tokenizer\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "# Customize training\n",
        "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
        "    \"<s>\",\n",
        "    \"<pad>\",\n",
        "    \"</s>\",\n",
        "    \"<unk>\",\n",
        "    \"<mask>\",\n",
        "])\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_wPE6Yf4rml"
      },
      "source": [
        "## √©tape 4 : enregistrer les fichiers sur le disque Le tokenizer g√©n√©rera deux fichiers une fois entra√Æn√©s : \n",
        "*  merges.txt, qui contient les sous-cha√Ænes fusionn√©es \n",
        "* vocab.json, qui contient les index des sous-cha√Ænes tokenis√©es \n",
        "\n",
        "Le programme cr√©e d'abord le r√©pertoire KantaiBERT puis enregistre les deux fichiers :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFEHEjNa48Rm",
        "outputId": "7531df26-746f-4b68-d2e8-308ba4cdec70"
      },
      "source": [
        "import os\n",
        "\n",
        "token_dir = '/content/KantaiBERT'\n",
        "\n",
        "if not os.path.exists(token_dir):\n",
        "  os.makedirs(token_dir)\n",
        "\n",
        "tokenizer.save_model('KantaiBERT')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['KantaiBERT/vocab.json', 'KantaiBERT/merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqSl-op25WKf"
      },
      "source": [
        "## √âtape 5 : Chargement des fichiers de tokenizer entra√Æn√©s\n",
        "\n",
        "Nous aurions pu charger des fichiers de tokenizer pr√©-entra√Æn√©s. Cependant, nous avons form√© notre propre tokenizer et sommes maintenant pr√™ts √† charger les fichiers :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUZq36QA5eYq"
      },
      "source": [
        "from tokenizers.implementations import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer(    \n",
        "    \"./KantaiBERT/vocab.json\",    \n",
        "    \"./KantaiBERT/merges.txt\",\n",
        "    )"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ6CaH4L5pdS"
      },
      "source": [
        "\n",
        "Le tokenizer peut encoder une s√©quence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z97hYq5d5uAf",
        "outputId": "3b049b1c-9887-484b-a970-5a6d79a8f3a6"
      },
      "source": [
        "tokenizer.encode(\"The Critique of Pure Reason.\").tokens"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'ƒ†Critique', 'ƒ†of', 'ƒ†Pure', 'ƒ†Reason', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH_eNXkQ6ChM"
      },
      "source": [
        "On peut aussi demander √† voir le nombre de jetons dans cette s√©quence :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zH_9wcPF6EIt",
        "outputId": "e9157ba4-debb-449c-ee28-d24b4f7e2f4b"
      },
      "source": [
        "tokenizer.encode(\"The Critique of Pure Reason.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoding(num_tokens=6, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rk5JNKXV6XR8"
      },
      "source": [
        "Le tokenizer traite maintenant les jetons pour s'adapter √† la variante du mod√®le BERT utilis√©e dans ce bloc-notes. Le post-processeur ajoutera un jeton de d√©but et de fin, par exemple"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLoCDDC96b9C"
      },
      "source": [
        "tokenizer._tokenizer.post_processor = BertProcessing(    \n",
        "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),    \n",
        "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
        "    )"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tY9lTe-6qgZ"
      },
      "source": [
        "tokenizer.enable_truncation(max_length=512)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFgkrUuv65KP"
      },
      "source": [
        "Codons une s√©quence post-trait√©e"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DS8cH6Co7BAv",
        "outputId": "d3d66884-472e-46b3-b180-a61472757eea"
      },
      "source": [
        "tokenizer.encode(\"The Critique of Pure Reason.\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoding(num_tokens=8, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmgWuV8Z7PUU"
      },
      "source": [
        "\n",
        "Si nous voulons voir ce qui a √©t√© ajout√©, nous pouvons demander au tokenizer d'encoder la s√©quence post-trait√©e en ex√©cutant la cellule suivante"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Caqxk-dd7P6j",
        "outputId": "fa426136-41eb-4987-d41f-009a65dc0670"
      },
      "source": [
        "tokenizer.encode(\"The Critique of Pure Reason.\").tokens"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>', 'The', 'ƒ†Critique', 'ƒ†of', 'ƒ†Pure', 'ƒ†Reason', '.', '</s>']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4lI8iRm7ffJ"
      },
      "source": [
        "## √âtape 6 : V√©rification des contraintes de ressources : GPU et CUDA\n",
        "\n",
        "KantaiBERT fonctionne √† une vitesse optimale avec une unit√© de traitement graphique (GPU). Nous ex√©cuterons d'abord une commande pour voir si une carte GPU NVIDIA est pr√©sente :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ix2TL3037s1O",
        "outputId": "4ec52e06-0966-434d-ad73-7d7e29af42b7"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov  8 10:10:10 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTjli4wp7xdI",
        "outputId": "f6a5fa2e-5284-44a2-cde5-2772993dbc38"
      },
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTy0Txav8A0B"
      },
      "source": [
        "## √âtape 7 : D√©finition de la configuration du mod√®le\n",
        "\n",
        "Nous pr√©formons un mod√®le de transformateur de type RoBERTa utilisant le m√™me nombre de couches et de t√™tes qu'un transformateur DistilBERT. Le mod√®le aura une taille de vocabulaire d√©finie sur 52 000, 12 t√™tes d'attention et 6 couches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUx8CxPm8Iih"
      },
      "source": [
        "from transformers import RobertaConfig\n",
        "\n",
        "config = RobertaConfig(\n",
        "    vocab_size=52_000,\n",
        "    max_position_embeddings=514,\n",
        "    num_attention_heads=12,\n",
        "    num_hidden_layers=6,\n",
        "    type_vocab_size=1,\n",
        ")\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3nnE7_-9CIF"
      },
      "source": [
        "\n",
        "Nous allons explorer la configuration plus en d√©tail √† l'√©tape 9 : Initialiser un mod√®le √† partir de z√©ro. Commen√ßons par recr√©er le tokenizer dans notre mod√®le."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b9X0DaO9eId"
      },
      "source": [
        "##  √âtape 8 : Recharger le tokenizer dans les transformateurs\n",
        "\n",
        "Nous sommes maintenant pr√™ts √† charger notre tokenizer form√©, qui est notre tokenizer pr√©-entra√Æn√© dans RobertaTokenizer.from_pretained():\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9rUKBhC9oN6",
        "outputId": "574f8346-6a6e-46ab-cb59-419d98928ad9"
      },
      "source": [
        "from transformers import RobertaTokenizer\n",
        " \n",
        "tokenizer = RobertaTokenizer.from_pretrained( \"./KantaiBERT\", max_length=512)\n",
        " "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "file ./KantaiBERT/config.json not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLKh2ptC9oV6"
      },
      "source": [
        "Maintenant que nous avons charg√© notre tokenizer form√©, initialisons un mod√®le RoBERTa √† partir de z√©ro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5PuOVCH-Azy"
      },
      "source": [
        "## √âtape 9 : Initialisation d'un mod√®le √† partir de z√©ro\n",
        "\n",
        "Dans cette section, nous allons initialiser un mod√®le √† partir de z√©ro et examiner la taille du mod√®le. Le programme importe d'abord un mod√®le masqu√© RoBERTa pour la mod√©lisation du langage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnQnzrX3-FET"
      },
      "source": [
        "from transformers import RobertaForMaskedLM\n",
        "\n",
        "model = RobertaForMaskedLM(config=config)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tINgA-Vt-4zM"
      },
      "source": [
        "## √âtape 10 : Construire l'ensemble de donn√©es \n",
        "\n",
        "Le programme va maintenant charger l'ensemble de donn√©es ligne par ligne pour l'apprentissage par lots avec block_size=128 limitant la longueur d'un exemple :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnbF0nAh_PiZ",
        "outputId": "5672ee22-3424-4734-ea76-1df0ca8a5e0c"
      },
      "source": [
        "from transformers import LineByLineTextDataset\n",
        "\n",
        "dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"./kant.txt\",\n",
        "    block_size=128,\n",
        ")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:125: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i65ukhvV_h76"
      },
      "source": [
        "## √âtape 11 : D√©finir un assembleur de donn√©es\n",
        "\n",
        "Nous devons ex√©cuter un assembleur de donn√©es avant d'initialiser le formateur.\n",
        "\n",
        " Un collecteur de donn√©es pr√©l√®vera des √©chantillons de l'ensemble de donn√©es et les rassemblera en lots. \n",
        " \n",
        " Les r√©sultats sont des objets de type dictionnaire. Nous pr√©parons un processus d'√©chantillonnage par lots pour la mod√©lisation du langage masqu√© (MLM) en d√©finissant mlm=True.\n",
        " \n",
        " Nous avons √©galement d√©fini le nombre de jetons masqu√©s pour entra√Æner mlm_probability=0.15. \n",
        " \n",
        " Cela d√©terminera le pourcentage de jetons masqu√©s pendant le processus de pr√©-entra√Ænement. Nous initialisons maintenant data_collator avec notre tokenizer, MLM activ√© et la proportion de jetons masqu√©s d√©finie sur 0,15"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgifjZ0r_-77"
      },
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
        ")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFrQBPSrAdh-"
      },
      "source": [
        "## √âtape 12 : Initialisation du formateur \n",
        "\n",
        "Les √©tapes pr√©c√©dentes ont pr√©par√© les informations n√©cessaires √† l'initialisation du formateur. \n",
        "\n",
        "L'ensemble de donn√©es a √©t√© tokenis√© et charg√©. Notre mod√®le est construit. Le collecteur de donn√©es a √©t√© cr√©√©. Le programme peut maintenant initialiser le formateur. √Ä des fins √©ducatives, le programme entra√Æne le mod√®le rapidement. Le nombre d'√©poques est limit√© √† un. Le GPU est pratique car nous pouvons partager les lots et multi-traiter les t√¢ches de formation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPO-s0srAmkM"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./EsperBERTo\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,\n",
        "    per_gpu_train_batch_size=64,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKSIhVZZBDPl"
      },
      "source": [
        "## √âtape 13 : Pr√©formation du mod√®le \n",
        "\n",
        "Tout est pr√™t. Le formateur est lanc√© avec une ligne de code :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509
        },
        "id": "vB6Y6TD1BKg9",
        "outputId": "b414c049-1685-4ff1-c879-337f94f745d6"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "***** Running training *****\n",
            "  Num examples = 170964\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 2672\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2672' max='2672' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2672/2672 19:37, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>6.606600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>5.711500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>5.226400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>4.978500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>4.824400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2672, training_loss=5.425474223976364, metrics={'train_runtime': 1178.5633, 'train_samples_per_second': 145.061, 'train_steps_per_second': 2.267, 'total_flos': 873620128952064.0, 'train_loss': 5.425474223976364, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRvFMIsYCBqR"
      },
      "source": [
        "## √âtape 14 : Sauvegarder le mod√®le final (+tokenizer + config) sur le disque\n",
        "\n",
        "Nous allons maintenant sauvegarder le mod√®le et la configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UHCSWUkCXl3",
        "outputId": "ed379aba-8f2e-44d4-ee77-f326a35c19ee"
      },
      "source": [
        "trainer.save_model(\"./KantaiBERT\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./KantaiBERT\n",
            "Configuration saved in ./KantaiBERT/config.json\n",
            "Model weights saved in ./KantaiBERT/pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL5PVR2pCXsC"
      },
      "source": [
        "## √âtape 15 : Mod√©lisation du langage avec FillMaskPipeline\n",
        "\n",
        "Nous allons maintenant importer une t√¢che de masque de remplissage de mod√©lisation du langage. Nous utiliserons notre mod√®le entra√Æn√© et notre tokenizer entra√Æn√© pour effectuer une mod√©lisation de langage masqu√©"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jx0Oe4gmC5R1",
        "outputId": "236e6196-360c-45cb-c063-e611072122bb"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "fill_mask = pipeline(    \n",
        "    \"fill-mask\",    \n",
        "    model=\"./KantaiBERT\",    \n",
        "    tokenizer=\"./KantaiBERT\"\n",
        "    )"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file ./KantaiBERT/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.13.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 52000\n",
            "}\n",
            "\n",
            "loading configuration file ./KantaiBERT/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.13.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 52000\n",
            "}\n",
            "\n",
            "loading weights file ./KantaiBERT/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
            "\n",
            "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at ./KantaiBERT.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file ./KantaiBERT/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.13.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 52000\n",
            "}\n",
            "\n",
            "Didn't find file ./KantaiBERT/tokenizer.json. We won't load it.\n",
            "Didn't find file ./KantaiBERT/added_tokens.json. We won't load it.\n",
            "Didn't find file ./KantaiBERT/special_tokens_map.json. We won't load it.\n",
            "Didn't find file ./KantaiBERT/tokenizer_config.json. We won't load it.\n",
            "loading file ./KantaiBERT/vocab.json\n",
            "loading file ./KantaiBERT/merges.txt\n",
            "loading file None\n",
            "loading file None\n",
            "loading file None\n",
            "loading file None\n",
            "loading configuration file ./KantaiBERT/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.13.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 52000\n",
            "}\n",
            "\n",
            "loading configuration file ./KantaiBERT/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.13.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 52000\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvYzQyZdDCO2",
        "outputId": "1e6d7df0-2944-4229-f61e-e90d9aff7591"
      },
      "source": [
        "fill_mask(\"Human thinking involves human <mask>.\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.04041118547320366,\n",
              "  'sequence': 'Human thinking involves human reason.',\n",
              "  'token': 393,\n",
              "  'token_str': ' reason'},\n",
              " {'score': 0.014237454161047935,\n",
              "  'sequence': 'Human thinking involves human experience.',\n",
              "  'token': 531,\n",
              "  'token_str': ' experience'},\n",
              " {'score': 0.009888945147395134,\n",
              "  'sequence': 'Human thinking involves human conceptions.',\n",
              "  'token': 605,\n",
              "  'token_str': ' conceptions'},\n",
              " {'score': 0.009337211959064007,\n",
              "  'sequence': 'Human thinking involves human it.',\n",
              "  'token': 306,\n",
              "  'token_str': ' it'},\n",
              " {'score': 0.007209516130387783,\n",
              "  'sequence': 'Human thinking involves human time.',\n",
              "  'token': 526,\n",
              "  'token_str': ' time'}]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}