{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pretraining_a_RoBERT_Model_from_Scratch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtCQAk5ps461"
      },
      "source": [
        "# Pretraining a RoBERTa Model from Scratch\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxop6UrAvVrl"
      },
      "source": [
        "## Training a tokenizer and pretraining a transformer\n",
        "\n",
        "Dans ce guide, nous allons entraîner un modèle de transformateur nommé KantaiBERT en utilisant les blocs de construction fournis par Hugging Face pour les modèles de type BERT. Nous avons couvert la théorie des éléments constitutifs du modèle que nous avons utilisé précédement.\n",
        "\n",
        "Nous décrirons KantaiBERT en nous appuyant sur les connaissances acquises dans les guides précédents\n",
        "\n",
        "KantaiBERT est un modèle de type Robustly Optimized BERT Pretraining Approach (RoBERTa) basé sur l'architecture de BERT. \n",
        "\n",
        "\n",
        "Les modèles BERT initiaux étaient sous-formés. RoBERTa augmente les performances des transformateurs de pré-apprentissage pour les tâches en aval. RoBERTa a amélioré la mécanique du processus de pré-formation. Par exemple, il n'utilise pas la tokenisation WordPiece mais descend à l'encodageByte Pair Encoding (BPE)\n",
        "\n",
        "Dans ce guide, KantaiBERT, comme BERT, sera formé à l'aide de la modélisation du langage masqué. KantaiBERT sera formé comme un petit modèle avec 6 couches, 12 têtes et 84 095 008 paramètres. Il peut sembler que 84 millions de paramètres représentent un grand nombre de paramètres. \n",
        "\n",
        "Cependant, les paramètres sont répartis sur 6 couches et 12 têtes, ce qui le rend relativement petit. Un petit modèle rendra l'expérience de pré-entraînement fluide afin que chaque étape puisse être visualisée en temps réel sans attendre des heures pour voir un résultat\n",
        "\n",
        "\n",
        "KantaiBERT est un modèle de type DistilBERT car il a la même architecture de 6 couches et 12 têtes. DistilBERT est une version distillée de BERT. Nous savons que les grands modèles offrent d'excellentes performances. Mais que faire si vous souhaitez exécuter un modèle sur un smartphone ? La miniaturisation a été la clé de l'évolution technologique. \n",
        "\n",
        "Les transformateurs devront suivre le même chemin lors de la mise en œuvre. L'approche Hugging Face utilisant une version distillée de BERT est donc un bon pas en avant. La distillation, ou d'autres méthodes de ce type à l'avenir, est un moyen intelligent de tirer le meilleur parti de la préformation et de la rendre efcace pour les besoins de nombreuses tâches en aval.\n",
        "\n",
        "KantaiBERT implémentera un tokenizer byte-level byte-pair encoding comme celui utilisé par GPT-2. Les jetons spéciaux seront ceux utilisés par RoBERTa.\n",
        "\n",
        "\n",
        "Il n'y a pas d'ID de type de jeton pour indiquer à quelle partie d'un segment un jeton fait partie. Les segments seront séparés avec le jeton de séparation \\</s>.\n",
        "\n",
        "KantaiBERT utilisera un ensemble de données personnalisé, formera un tokenizer, formera le modèle de transformateur, l'enregistrera et l'exécutera avec un exemple de modélisation de langage masqué.\n",
        "\n",
        "Commençons à construire un transformateur de zéro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzuVQsbsyqxP"
      },
      "source": [
        "## Étape 1 : Chargement de l'ensemble de données \n",
        "\n",
        "Les ensembles de données prêts à l'emploi offrent un moyen objectif d'entraîner et de comparer les transformateurs. \n",
        " \n",
        " Le but de ce guide est de comprendre le processus d'apprentissage d'un transformateur avec des cellules de bloc-notes qui pourrait être exécuté en temps réel sans avoir à attendre des heures pour obtenir un résultat.\n",
        "\n",
        " \n",
        "J'ai choisi d'utiliser les œuvres d'Emmanuel Kant (1724-1804), le philosophe allemand, qui fut l'incarnation du siècle des Lumières. L'idée est d'introduire une logique humaine et un raisonnement pré-entraîné pour les tâches de raisonnement en aval.\n",
        "\n",
        "\n",
        "Project Gutenberg, https://www.gutenberg.org, propose une large gamme de livres électroniques gratuits qui peuvent être téléchargés au format texte. \n",
        "\n",
        "Vous pouvez utiliser d'autres livres si vous souhaitez créer vos propres ensembles de données personnalisés basés sur des livres. J'ai compilé les trois livres suivants d'Immanuel Kant dans un fichier texte nommé kant.txt :\n",
        "\n",
        "\n",
        "* he Critique of Pure Reason\n",
        "* The Critique of Practical Reason\n",
        "* Fundamental Principles of the Metaphysic of Morals\n",
        "\n",
        "kant.txt fournit un petit ensemble de données d'entraînement pour entraîner le modèle de transformateur de ce guide. \n",
        "\n",
        "Le résultat obtenu reste expérimental. Pour un projet réel.\n",
        "\n",
        "L'ensemble de données est téléchargé automatiquement depuis GitHub :\n",
        "\n",
        "vous pouvez utiliser curl pour le récupérer depuis GitHub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDddOVc00CBU",
        "outputId": "5362acbd-7161-4737-e4c7-c12ed58aa6b8"
      },
      "source": [
        "!curl -L https://raw.githubusercontent.com/PacktPublishing/Transformers-for-Natural-Language-Processing/master/Chapter03/kant.txt --output \"kant.txt\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 10.7M  100 10.7M    0     0  20.5M      0 --:--:-- --:--:-- --:--:-- 20.4M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJdyXdIW0Qrc"
      },
      "source": [
        "## Step 2: Installation Hugging Face transformersWe \n",
        "\n",
        "Nous devrons installer des transformateurs et des tokenizers Hugging Face, mais nous n'aurons pas besoin de TensorFlow dans cette instance de la VM Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzyKfqoK0pkf",
        "outputId": "feaf9c32-5f76-4bee-e0ea-b40343ae1bf9"
      },
      "source": [
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip list | grep -E 'transformers|tokenizers'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-wnwaxpi1\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-wnwaxpi1\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (1.19.5)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 31.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (3.3.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 44.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (4.8.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (21.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (2019.12.20)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.1.1-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.13.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.13.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.13.0.dev0) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13.0.dev0) (7.1.2)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.13.0.dev0-py3-none-any.whl size=3101540 sha256=c578e02210150d279d091b52e3928a5730f8ede84d27c5bae83a1d270793cf7d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-snvj8l71/wheels/35/2e/a7/d819e3310040329f0f47e57c9e3e7a7338aa5e74c49acfe522\n",
            "Successfully built transformers\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.1.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.13.0.dev0\n",
            "tokenizers                    0.10.3\n",
            "transformers                  4.13.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjFGMp2T02_x"
      },
      "source": [
        "## Étape 3 : Formation d'un tokenizer\n",
        "\n",
        "Dans cette section, le programme n'utilise pas de tokenizer pré-entraîné. Par exemple, un tokenizer GPT-2 pré-entraîné pourrait être utilisé. Cependant, le processus de formation de ce chapitre comprend la formation d'un tokenizer à partir de zéro. ByteLevelBPETokenizer() de Hugging Face sera formé à l'aide de kant.txt. \n",
        "\n",
        "Un tokenizer au niveau de l'octet divisera une chaîne ou un mot en une sous-chaîne ou un sous-mot. Il y a deux avantages principaux parmi tant d'autres\n",
        "\n",
        "* Le tokenizer peut diviser les mots en composants minimaux. Ensuite, il fusionnera ces petits composants en d'autres statistiquement intéressants. Par exemple, \"smaller\" et smallest\" peuvent devenir \"small\", \"er\" et \"est\".\n",
        "\n",
        "Le tokenizer peut aller plus loin, et on pourrait obtenir « sm » et « all », par exemple. Dans tous les cas, les mots sont décomposés en jetons de sous-mots et en unités plus petites de parties de sous-mots telles que \"sm\" et \"all\" au lieu de simplement \"small\".\n",
        "\n",
        "* Les morceaux de chaînes classés comme un jeton inconnu, utilisant l'encodage de niveau WorkPiece, disparaîtront pratiquement.\n",
        "\n",
        "\n",
        "Dans ce modèle, nous allons entraîner le tokenizer avec les paramètres suivants :\n",
        "\n",
        "* files=paths est le chemin d'accès à l'ensemble de données.\n",
        "* vocab_size=52_000 est la taille de la longueur du modèle de notre tokenizer•\n",
        "* min_fréquence=2 est le seuil de fréquence minimum.\n",
        "* special_tokens=[] est une liste de jetons spéciaux\n",
        "\n",
        "\n",
        "Dans ce cas, la liste des jetons spéciaux est :\n",
        "* \\<s> : un jeton de début\n",
        "* \\<pad> : un jeton de remplissage\n",
        "* \\</s> : un jeton de fin\n",
        "* \\<unk> : un jeton inconnu\n",
        "* \\<mask> : le jeton de masque pour la modélisation du langage\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9u4scPP3-q3"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "paths = [str(x) for x in Path(\".\").glob(\"**/*.txt\")]\n",
        "\n",
        "# Initialize a tokenizer\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "# Customize training\n",
        "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
        "    \"<s>\",\n",
        "    \"<pad>\",\n",
        "    \"</s>\",\n",
        "    \"<unk>\",\n",
        "    \"<mask>\",\n",
        "])\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_wPE6Yf4rml"
      },
      "source": [
        "## étape 4 : enregistrer les fichiers sur le disque Le tokenizer générera deux fichiers une fois entraînés : \n",
        "*  merges.txt, qui contient les sous-chaînes fusionnées \n",
        "* vocab.json, qui contient les index des sous-chaînes tokenisées \n",
        "\n",
        "Le programme crée d'abord le répertoire KantaiBERT puis enregistre les deux fichiers :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFEHEjNa48Rm",
        "outputId": "7531df26-746f-4b68-d2e8-308ba4cdec70"
      },
      "source": [
        "import os\n",
        "\n",
        "token_dir = '/content/KantaiBERT'\n",
        "\n",
        "if not os.path.exists(token_dir):\n",
        "  os.makedirs(token_dir)\n",
        "\n",
        "tokenizer.save_model('KantaiBERT')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['KantaiBERT/vocab.json', 'KantaiBERT/merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqSl-op25WKf"
      },
      "source": [
        "## Étape 5 : Chargement des fichiers de tokenizer entraînés\n",
        "\n",
        "Nous aurions pu charger des fichiers de tokenizer pré-entraînés. Cependant, nous avons formé notre propre tokenizer et sommes maintenant prêts à charger les fichiers :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUZq36QA5eYq"
      },
      "source": [
        "from tokenizers.implementations import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer(    \n",
        "    \"./KantaiBERT/vocab.json\",    \n",
        "    \"./KantaiBERT/merges.txt\",\n",
        "    )"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ6CaH4L5pdS"
      },
      "source": [
        "\n",
        "Le tokenizer peut encoder une séquence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z97hYq5d5uAf",
        "outputId": "3b049b1c-9887-484b-a970-5a6d79a8f3a6"
      },
      "source": [
        "tokenizer.encode(\"The Critique of Pure Reason.\").tokens"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'ĠCritique', 'Ġof', 'ĠPure', 'ĠReason', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH_eNXkQ6ChM"
      },
      "source": [
        "On peut aussi demander à voir le nombre de jetons dans cette séquence :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zH_9wcPF6EIt",
        "outputId": "e9157ba4-debb-449c-ee28-d24b4f7e2f4b"
      },
      "source": [
        "tokenizer.encode(\"The Critique of Pure Reason.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoding(num_tokens=6, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rk5JNKXV6XR8"
      },
      "source": [
        "Le tokenizer traite maintenant les jetons pour s'adapter à la variante du modèle BERT utilisée dans ce bloc-notes. Le post-processeur ajoutera un jeton de début et de fin, par exemple"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLoCDDC96b9C"
      },
      "source": [
        "tokenizer._tokenizer.post_processor = BertProcessing(    \n",
        "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),    \n",
        "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
        "    )"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tY9lTe-6qgZ"
      },
      "source": [
        "tokenizer.enable_truncation(max_length=512)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFgkrUuv65KP"
      },
      "source": [
        "Codons une séquence post-traitée"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DS8cH6Co7BAv",
        "outputId": "d3d66884-472e-46b3-b180-a61472757eea"
      },
      "source": [
        "tokenizer.encode(\"The Critique of Pure Reason.\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoding(num_tokens=8, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmgWuV8Z7PUU"
      },
      "source": [
        "\n",
        "Si nous voulons voir ce qui a été ajouté, nous pouvons demander au tokenizer d'encoder la séquence post-traitée en exécutant la cellule suivante"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Caqxk-dd7P6j",
        "outputId": "fa426136-41eb-4987-d41f-009a65dc0670"
      },
      "source": [
        "tokenizer.encode(\"The Critique of Pure Reason.\").tokens"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>', 'The', 'ĠCritique', 'Ġof', 'ĠPure', 'ĠReason', '.', '</s>']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4lI8iRm7ffJ"
      },
      "source": [
        "## Étape 6 : Vérification des contraintes de ressources : GPU et CUDA\n",
        "\n",
        "KantaiBERT fonctionne à une vitesse optimale avec une unité de traitement graphique (GPU). Nous exécuterons d'abord une commande pour voir si une carte GPU NVIDIA est présente :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ix2TL3037s1O",
        "outputId": "4ec52e06-0966-434d-ad73-7d7e29af42b7"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov  8 10:10:10 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTjli4wp7xdI",
        "outputId": "f6a5fa2e-5284-44a2-cde5-2772993dbc38"
      },
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTy0Txav8A0B"
      },
      "source": [
        "## Étape 7 : Définition de la configuration du modèle\n",
        "\n",
        "Nous préformons un modèle de transformateur de type RoBERTa utilisant le même nombre de couches et de têtes qu'un transformateur DistilBERT. Le modèle aura une taille de vocabulaire définie sur 52 000, 12 têtes d'attention et 6 couches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUx8CxPm8Iih"
      },
      "source": [
        "from transformers import RobertaConfig\n",
        "\n",
        "config = RobertaConfig(\n",
        "    vocab_size=52_000,\n",
        "    max_position_embeddings=514,\n",
        "    num_attention_heads=12,\n",
        "    num_hidden_layers=6,\n",
        "    type_vocab_size=1,\n",
        ")\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3nnE7_-9CIF"
      },
      "source": [
        "\n",
        "Nous allons explorer la configuration plus en détail à l'étape 9 : Initialiser un modèle à partir de zéro. Commençons par recréer le tokenizer dans notre modèle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b9X0DaO9eId"
      },
      "source": [
        "##  Étape 8 : Recharger le tokenizer dans les transformateurs\n",
        "\n",
        "Nous sommes maintenant prêts à charger notre tokenizer formé, qui est notre tokenizer pré-entraîné dans RobertaTokenizer.from_pretained():\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9rUKBhC9oN6",
        "outputId": "574f8346-6a6e-46ab-cb59-419d98928ad9"
      },
      "source": [
        "from transformers import RobertaTokenizer\n",
        " \n",
        "tokenizer = RobertaTokenizer.from_pretrained( \"./KantaiBERT\", max_length=512)\n",
        " "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "file ./KantaiBERT/config.json not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLKh2ptC9oV6"
      },
      "source": [
        "Maintenant que nous avons chargé notre tokenizer formé, initialisons un modèle RoBERTa à partir de zéro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5PuOVCH-Azy"
      },
      "source": [
        "## Étape 9 : Initialisation d'un modèle à partir de zéro\n",
        "\n",
        "Dans cette section, nous allons initialiser un modèle à partir de zéro et examiner la taille du modèle. Le programme importe d'abord un modèle masqué RoBERTa pour la modélisation du langage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnQnzrX3-FET"
      },
      "source": [
        "from transformers import RobertaForMaskedLM\n",
        "\n",
        "model = RobertaForMaskedLM(config=config)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tINgA-Vt-4zM"
      },
      "source": [
        "## Étape 10 : Construire l'ensemble de données \n",
        "\n",
        "Le programme va maintenant charger l'ensemble de données ligne par ligne pour l'apprentissage par lots avec block_size=128 limitant la longueur d'un exemple :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnbF0nAh_PiZ",
        "outputId": "5672ee22-3424-4734-ea76-1df0ca8a5e0c"
      },
      "source": [
        "from transformers import LineByLineTextDataset\n",
        "\n",
        "dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"./kant.txt\",\n",
        "    block_size=128,\n",
        ")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:125: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i65ukhvV_h76"
      },
      "source": [
        "## Étape 11 : Définir un assembleur de données\n",
        "\n",
        "Nous devons exécuter un assembleur de données avant d'initialiser le formateur.\n",
        "\n",
        " Un collecteur de données prélèvera des échantillons de l'ensemble de données et les rassemblera en lots. \n",
        " \n",
        " Les résultats sont des objets de type dictionnaire. Nous préparons un processus d'échantillonnage par lots pour la modélisation du langage masqué (MLM) en définissant mlm=True.\n",
        " \n",
        " Nous avons également défini le nombre de jetons masqués pour entraîner mlm_probability=0.15. \n",
        " \n",
        " Cela déterminera le pourcentage de jetons masqués pendant le processus de pré-entraînement. Nous initialisons maintenant data_collator avec notre tokenizer, MLM activé et la proportion de jetons masqués définie sur 0,15"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgifjZ0r_-77"
      },
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
        ")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFrQBPSrAdh-"
      },
      "source": [
        "## Étape 12 : Initialisation du formateur \n",
        "\n",
        "Les étapes précédentes ont préparé les informations nécessaires à l'initialisation du formateur. \n",
        "\n",
        "L'ensemble de données a été tokenisé et chargé. Notre modèle est construit. Le collecteur de données a été créé. Le programme peut maintenant initialiser le formateur. À des fins éducatives, le programme entraîne le modèle rapidement. Le nombre d'époques est limité à un. Le GPU est pratique car nous pouvons partager les lots et multi-traiter les tâches de formation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPO-s0srAmkM"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./EsperBERTo\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,\n",
        "    per_gpu_train_batch_size=64,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKSIhVZZBDPl"
      },
      "source": [
        "## Étape 13 : Préformation du modèle \n",
        "\n",
        "Tout est prêt. Le formateur est lancé avec une ligne de code :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509
        },
        "id": "vB6Y6TD1BKg9",
        "outputId": "b414c049-1685-4ff1-c879-337f94f745d6"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "***** Running training *****\n",
            "  Num examples = 170964\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 2672\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2672' max='2672' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2672/2672 19:37, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>6.606600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>5.711500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>5.226400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>4.978500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>4.824400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2672, training_loss=5.425474223976364, metrics={'train_runtime': 1178.5633, 'train_samples_per_second': 145.061, 'train_steps_per_second': 2.267, 'total_flos': 873620128952064.0, 'train_loss': 5.425474223976364, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRvFMIsYCBqR"
      },
      "source": [
        "## Étape 14 : Sauvegarder le modèle final (+tokenizer + config) sur le disque\n",
        "\n",
        "Nous allons maintenant sauvegarder le modèle et la configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UHCSWUkCXl3",
        "outputId": "ed379aba-8f2e-44d4-ee77-f326a35c19ee"
      },
      "source": [
        "trainer.save_model(\"./KantaiBERT\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./KantaiBERT\n",
            "Configuration saved in ./KantaiBERT/config.json\n",
            "Model weights saved in ./KantaiBERT/pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL5PVR2pCXsC"
      },
      "source": [
        "## Étape 15 : Modélisation du langage avec FillMaskPipeline\n",
        "\n",
        "Nous allons maintenant importer une tâche de masque de remplissage de modélisation du langage. Nous utiliserons notre modèle entraîné et notre tokenizer entraîné pour effectuer une modélisation de langage masqué"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jx0Oe4gmC5R1",
        "outputId": "236e6196-360c-45cb-c063-e611072122bb"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "fill_mask = pipeline(    \n",
        "    \"fill-mask\",    \n",
        "    model=\"./KantaiBERT\",    \n",
        "    tokenizer=\"./KantaiBERT\"\n",
        "    )"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file ./KantaiBERT/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.13.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 52000\n",
            "}\n",
            "\n",
            "loading configuration file ./KantaiBERT/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.13.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 52000\n",
            "}\n",
            "\n",
            "loading weights file ./KantaiBERT/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
            "\n",
            "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at ./KantaiBERT.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file ./KantaiBERT/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.13.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 52000\n",
            "}\n",
            "\n",
            "Didn't find file ./KantaiBERT/tokenizer.json. We won't load it.\n",
            "Didn't find file ./KantaiBERT/added_tokens.json. We won't load it.\n",
            "Didn't find file ./KantaiBERT/special_tokens_map.json. We won't load it.\n",
            "Didn't find file ./KantaiBERT/tokenizer_config.json. We won't load it.\n",
            "loading file ./KantaiBERT/vocab.json\n",
            "loading file ./KantaiBERT/merges.txt\n",
            "loading file None\n",
            "loading file None\n",
            "loading file None\n",
            "loading file None\n",
            "loading configuration file ./KantaiBERT/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.13.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 52000\n",
            "}\n",
            "\n",
            "loading configuration file ./KantaiBERT/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.13.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 52000\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvYzQyZdDCO2",
        "outputId": "1e6d7df0-2944-4229-f61e-e90d9aff7591"
      },
      "source": [
        "fill_mask(\"Human thinking involves human <mask>.\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.04041118547320366,\n",
              "  'sequence': 'Human thinking involves human reason.',\n",
              "  'token': 393,\n",
              "  'token_str': ' reason'},\n",
              " {'score': 0.014237454161047935,\n",
              "  'sequence': 'Human thinking involves human experience.',\n",
              "  'token': 531,\n",
              "  'token_str': ' experience'},\n",
              " {'score': 0.009888945147395134,\n",
              "  'sequence': 'Human thinking involves human conceptions.',\n",
              "  'token': 605,\n",
              "  'token_str': ' conceptions'},\n",
              " {'score': 0.009337211959064007,\n",
              "  'sequence': 'Human thinking involves human it.',\n",
              "  'token': 306,\n",
              "  'token_str': ' it'},\n",
              " {'score': 0.007209516130387783,\n",
              "  'sequence': 'Human thinking involves human time.',\n",
              "  'token': 526,\n",
              "  'token_str': ' time'}]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}